# Tokenization Project

This repository contains a simple Python implementation of tokenization.  
The tokenizer splits text into words using regular expressions, with optional case normalization.

## Features
- Converts text to lowercase (optional).
- Splits text into words using regex (`\b\w+\b`).
- Minimal, single‑file implementation for easy use.

## Usage
Clone the repository and run:

YOU CAN ALSO PASS CIUSTOM TEXT:

```bash
python tokenizer.py

**EXAMPLE OUT PUT**

python tokenizer.py "This is my custom text"

**Project Structure**
tokenization/
├── tokenizer.py
└── README.md

**License**
This project is open‑source. Feel free to use and modify.


